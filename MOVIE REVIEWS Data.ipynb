{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## movie reviews \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I saw this movie when I was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Some films just simply should not be remade. T...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This movie made it into one of my top 10 most ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I remember this film,it was the first film i h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>An awful film! It must have been up against so...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>After the success of Die Hard and it's sequels...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I had the terrible misfortune of having to vie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>First of all, let's get a few things straight ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This was the worst movie I saw at WorldFest an...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Karen Carpenter Story shows a little more ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"The Cell\" is an exotic masterpiece, a dizzyin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This film tried to be too many things all at o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>'War movie' is a Hollywood genre that has been...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   One of the other reviewers has mentioned that ...  positive\n",
       "1   A wonderful little production. <br /><br />The...  positive\n",
       "2   I thought this was a wonderful way to spend ti...  positive\n",
       "3   Basically there's a family where a little boy ...  negative\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5   Probably my all-time favorite movie, a story o...  positive\n",
       "6   I sure would like to see a resurrection of a u...  positive\n",
       "7   This show was an amazing, fresh & innovative i...  negative\n",
       "8   Encouraged by the positive comments about this...  negative\n",
       "9   If you like original gut wrenching laughter yo...  positive\n",
       "10  Phil the Alien is one of those quirky films wh...  negative\n",
       "11  I saw this movie when I was about 12 when it c...  negative\n",
       "12  So im not a big fan of Boll's work but then ag...  negative\n",
       "13  The cast played Shakespeare.<br /><br />Shakes...  negative\n",
       "14  This a fantastic movie of three prisoners who ...  positive\n",
       "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
       "16  Some films just simply should not be remade. T...  positive\n",
       "17  This movie made it into one of my top 10 most ...  negative\n",
       "18  I remember this film,it was the first film i h...  positive\n",
       "19  An awful film! It must have been up against so...  negative\n",
       "20  After the success of Die Hard and it's sequels...  positive\n",
       "21  I had the terrible misfortune of having to vie...  negative\n",
       "22  What an absolutely stunning movie, if you have...  positive\n",
       "23  First of all, let's get a few things straight ...  negative\n",
       "24  This was the worst movie I saw at WorldFest an...  negative\n",
       "25  The Karen Carpenter Story shows a little more ...  positive\n",
       "26  \"The Cell\" is an exotic masterpiece, a dizzyin...  positive\n",
       "27  This film tried to be too many things all at o...  negative\n",
       "28  This movie was so frustrating. Everything seem...  negative\n",
       "29  'War movie' is a Hollywood genre that has been...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv',error_bad_lines=False)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=df['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of HTML contents like \"< br>\".\n",
    "Removal of punctutions, special characters like '\\'.\n",
    "Removal of stopwords like is, the which do not offer much insight.\n",
    "Stemming/Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.\n",
    "Vectorization - Encode the numeric values once you have cleaned it.\n",
    "Fit the data to the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=BeautifulSoup(review,'html.parser')\n",
    "soup=df1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##so in the next step we will remove everything except lower/upper case letters using Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just   Oz episode you ll be hooked  They are right  as this is exactly what happened with me  br    br   The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word  br    br   It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many  Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more    so scuffles  death stares  dodgy dealings and shady agreements are never far away  br    br   I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance   OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing    thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "review = re.sub('\\[[^]]*\\]', ' ',review )\n",
    "review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the other reviewers has mentioned that after watching just   oz episode you ll be hooked  they are right  as this is exactly what happened with me  br    br   the first thing that struck me about oz was its brutality and unflinching scenes of violence  which set in right from the word go  trust me  this is not a show for the faint hearted or timid  this show pulls no punches with regards to drugs  sex or violence  its is hardcore  in the classic use of the word  br    br   it is called oz as that is the nickname given to the oswald maximum security state penitentary  it focuses mainly on emerald city  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  em city is home to many  aryans  muslims  gangstas  latinos  christians  italians  irish and more    so scuffles  death stares  dodgy dealings and shady agreements are never far away  br    br   i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  forget pretty pictures painted for mainstream audiences  forget charm  forget romance   oz doesn t mess around  the first episode i ever saw struck me as so nasty it was surreal  i couldn t say i was ready for it  but as i watched more  i developed a taste for oz  and got accustomed to the high levels of graphic violence  not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  watching oz  you may become comfortable with what is uncomfortable viewing    thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopw=stopwords.words('english')\n",
    "stopw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=review.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching',\n",
       " 'just',\n",
       " 'Oz',\n",
       " 'episode',\n",
       " 'you',\n",
       " 'll',\n",
       " 'be',\n",
       " 'hooked',\n",
       " 'They',\n",
       " 'are',\n",
       " 'right',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'with',\n",
       " 'me',\n",
       " 'br',\n",
       " 'br',\n",
       " 'The',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'about',\n",
       " 'Oz',\n",
       " 'was',\n",
       " 'its',\n",
       " 'brutality',\n",
       " 'and',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'which',\n",
       " 'set',\n",
       " 'in',\n",
       " 'right',\n",
       " 'from',\n",
       " 'the',\n",
       " 'word',\n",
       " 'GO',\n",
       " 'Trust',\n",
       " 'me',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'show',\n",
       " 'for',\n",
       " 'the',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'or',\n",
       " 'timid',\n",
       " 'This',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'no',\n",
       " 'punches',\n",
       " 'with',\n",
       " 'regards',\n",
       " 'to',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'or',\n",
       " 'violence',\n",
       " 'Its',\n",
       " 'is',\n",
       " 'hardcore',\n",
       " 'in',\n",
       " 'the',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " 'br',\n",
       " 'br',\n",
       " 'It',\n",
       " 'is',\n",
       " 'called',\n",
       " 'OZ',\n",
       " 'as',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Oswald',\n",
       " 'Maximum',\n",
       " 'Security',\n",
       " 'State',\n",
       " 'Penitentary',\n",
       " 'It',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'on',\n",
       " 'Emerald',\n",
       " 'City',\n",
       " 'an',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'of',\n",
       " 'the',\n",
       " 'prison',\n",
       " 'where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'cells',\n",
       " 'have',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'and',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'so',\n",
       " 'privacy',\n",
       " 'is',\n",
       " 'not',\n",
       " 'high',\n",
       " 'on',\n",
       " 'the',\n",
       " 'agenda',\n",
       " 'Em',\n",
       " 'City',\n",
       " 'is',\n",
       " 'home',\n",
       " 'to',\n",
       " 'many',\n",
       " 'Aryans',\n",
       " 'Muslims',\n",
       " 'gangstas',\n",
       " 'Latinos',\n",
       " 'Christians',\n",
       " 'Italians',\n",
       " 'Irish',\n",
       " 'and',\n",
       " 'more',\n",
       " 'so',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'and',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'are',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'br',\n",
       " 'br',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'the',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'of',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'where',\n",
       " 'other',\n",
       " 'shows',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'dare',\n",
       " 'Forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'for',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'OZ',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'The',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'as',\n",
       " 'so',\n",
       " 'nasty',\n",
       " 'it',\n",
       " 'was',\n",
       " 'surreal',\n",
       " 'I',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'say',\n",
       " 'I',\n",
       " 'was',\n",
       " 'ready',\n",
       " 'for',\n",
       " 'it',\n",
       " 'but',\n",
       " 'as',\n",
       " 'I',\n",
       " 'watched',\n",
       " 'more',\n",
       " 'I',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'taste',\n",
       " 'for',\n",
       " 'Oz',\n",
       " 'and',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'Not',\n",
       " 'just',\n",
       " 'violence',\n",
       " 'but',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'who',\n",
       " 'll',\n",
       " 'be',\n",
       " 'sold',\n",
       " 'out',\n",
       " 'for',\n",
       " 'a',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'who',\n",
       " 'll',\n",
       " 'kill',\n",
       " 'on',\n",
       " 'order',\n",
       " 'and',\n",
       " 'get',\n",
       " 'away',\n",
       " 'with',\n",
       " 'it',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'being',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'to',\n",
       " 'their',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'or',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'Watching',\n",
       " 'Oz',\n",
       " 'you',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'what',\n",
       " 'is',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " 'get',\n",
       " 'in',\n",
       " 'touch',\n",
       " 'with',\n",
       " 'your',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'reviewers',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'Oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'They',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'br',\n",
       " 'br',\n",
       " 'The',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'Oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'GO',\n",
       " 'Trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'This',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'punches',\n",
       " 'regards',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'Its',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'word',\n",
       " 'br',\n",
       " 'br',\n",
       " 'It',\n",
       " 'called',\n",
       " 'OZ',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'Oswald',\n",
       " 'Maximum',\n",
       " 'Security',\n",
       " 'State',\n",
       " 'Penitentary',\n",
       " 'It',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'Emerald',\n",
       " 'City',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cells',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'Em',\n",
       " 'City',\n",
       " 'home',\n",
       " 'many',\n",
       " 'Aryans',\n",
       " 'Muslims',\n",
       " 'gangstas',\n",
       " 'Latinos',\n",
       " 'Christians',\n",
       " 'Italians',\n",
       " 'Irish',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'br',\n",
       " 'br',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'dare',\n",
       " 'Forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'OZ',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'The',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'I',\n",
       " 'say',\n",
       " 'I',\n",
       " 'ready',\n",
       " 'I',\n",
       " 'watched',\n",
       " 'I',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'Oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'Not',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'Watching',\n",
       " 'Oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = [word for word in review if not word in set(stopw)]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'review',\n",
       " 'mention',\n",
       " 'watch',\n",
       " 'Oz',\n",
       " 'episod',\n",
       " 'hook',\n",
       " 'they',\n",
       " 'right',\n",
       " 'exactli',\n",
       " 'happen',\n",
       " 'br',\n",
       " 'br',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'Oz',\n",
       " 'brutal',\n",
       " 'unflinch',\n",
       " 'scene',\n",
       " 'violenc',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'GO',\n",
       " 'trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'heart',\n",
       " 'timid',\n",
       " 'thi',\n",
       " 'show',\n",
       " 'pull',\n",
       " 'punch',\n",
       " 'regard',\n",
       " 'drug',\n",
       " 'sex',\n",
       " 'violenc',\n",
       " 'it',\n",
       " 'hardcor',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'word',\n",
       " 'br',\n",
       " 'br',\n",
       " 'It',\n",
       " 'call',\n",
       " 'OZ',\n",
       " 'nicknam',\n",
       " 'given',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'secur',\n",
       " 'state',\n",
       " 'penitentari',\n",
       " 'It',\n",
       " 'focus',\n",
       " 'mainli',\n",
       " 'emerald',\n",
       " 'citi',\n",
       " 'experiment',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cell',\n",
       " 'glass',\n",
       " 'front',\n",
       " 'face',\n",
       " 'inward',\n",
       " 'privaci',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'Em',\n",
       " 'citi',\n",
       " 'home',\n",
       " 'mani',\n",
       " 'aryan',\n",
       " 'muslim',\n",
       " 'gangsta',\n",
       " 'latino',\n",
       " 'christian',\n",
       " 'italian',\n",
       " 'irish',\n",
       " 'scuffl',\n",
       " 'death',\n",
       " 'stare',\n",
       " 'dodgi',\n",
       " 'deal',\n",
       " 'shadi',\n",
       " 'agreement',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'br',\n",
       " 'br',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goe',\n",
       " 'show',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretti',\n",
       " 'pictur',\n",
       " 'paint',\n",
       " 'mainstream',\n",
       " 'audienc',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romanc',\n",
       " 'OZ',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'the',\n",
       " 'first',\n",
       " 'episod',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasti',\n",
       " 'surreal',\n",
       " 'I',\n",
       " 'say',\n",
       " 'I',\n",
       " 'readi',\n",
       " 'I',\n",
       " 'watch',\n",
       " 'I',\n",
       " 'develop',\n",
       " 'tast',\n",
       " 'Oz',\n",
       " 'got',\n",
       " 'accustom',\n",
       " 'high',\n",
       " 'level',\n",
       " 'graphic',\n",
       " 'violenc',\n",
       " 'not',\n",
       " 'violenc',\n",
       " 'injustic',\n",
       " 'crook',\n",
       " 'guard',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmat',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'manner',\n",
       " 'middl',\n",
       " 'class',\n",
       " 'inmat',\n",
       " 'turn',\n",
       " 'prison',\n",
       " 'bitch',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skill',\n",
       " 'prison',\n",
       " 'experi',\n",
       " 'watch',\n",
       " 'Oz',\n",
       " 'may',\n",
       " 'becom',\n",
       " 'comfort',\n",
       " 'uncomfort',\n",
       " 'view',\n",
       " 'that',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "review_s = [ps.stem(word) for word in review]\n",
    "review_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'reviewer',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'Oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'They',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'br',\n",
       " 'br',\n",
       " 'The',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'Oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scene',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'GO',\n",
       " 'Trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'This',\n",
       " 'show',\n",
       " 'pull',\n",
       " 'punch',\n",
       " 'regard',\n",
       " 'drug',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'Its',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'word',\n",
       " 'br',\n",
       " 'br',\n",
       " 'It',\n",
       " 'called',\n",
       " 'OZ',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'Oswald',\n",
       " 'Maximum',\n",
       " 'Security',\n",
       " 'State',\n",
       " 'Penitentary',\n",
       " 'It',\n",
       " 'focus',\n",
       " 'mainly',\n",
       " 'Emerald',\n",
       " 'City',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cell',\n",
       " 'glass',\n",
       " 'front',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'Em',\n",
       " 'City',\n",
       " 'home',\n",
       " 'many',\n",
       " 'Aryans',\n",
       " 'Muslims',\n",
       " 'gangsta',\n",
       " 'Latinos',\n",
       " 'Christians',\n",
       " 'Italians',\n",
       " 'Irish',\n",
       " 'scuffle',\n",
       " 'death',\n",
       " 'stare',\n",
       " 'dodgy',\n",
       " 'dealing',\n",
       " 'shady',\n",
       " 'agreement',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'br',\n",
       " 'br',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'go',\n",
       " 'show',\n",
       " 'dare',\n",
       " 'Forget',\n",
       " 'pretty',\n",
       " 'picture',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audience',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'OZ',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'The',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'I',\n",
       " 'say',\n",
       " 'I',\n",
       " 'ready',\n",
       " 'I',\n",
       " 'watched',\n",
       " 'I',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'Oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'level',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'Not',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guard',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmate',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmate',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitch',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skill',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'Watching',\n",
       " 'Oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "review = [lem.lemmatize(word) for word in review]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One reviewer mentioned watching Oz episode hooked They right exactly happened br br The first thing struck Oz brutality unflinching scene violence set right word GO Trust show faint hearted timid This show pull punch regard drug sex violence Its hardcore classic use word br br It called OZ nickname given Oswald Maximum Security State Penitentary It focus mainly Emerald City experimental section prison cell glass front face inwards privacy high agenda Em City home many Aryans Muslims gangsta Latinos Christians Italians Irish scuffle death stare dodgy dealing shady agreement never far away br br I would say main appeal show due fact go show dare Forget pretty picture painted mainstream audience forget charm forget romance OZ mess around The first episode I ever saw struck nasty surreal I say I ready I watched I developed taste Oz got accustomed high level graphic violence Not violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience Watching Oz may become comfortable uncomfortable viewing thats get touch darker side'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Our next step will be to bring this text in mathematical forms and to do so we will create a Corpus first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the text we will apply -\n",
    "CountVectorizer (Bag of Words Model)\n",
    "TfidfVectorizer (Bag of Words Model)\n",
    "Keras Tokenizer (Embedding)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1,\n",
       "        1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        6, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 4, 1, 2, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec=CountVectorizer()\n",
    "review_count_vec=count_vec.fit_transform(corpus)\n",
    "review_count_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########So we can see the data has become numeric with 1,2 and 3s based on the number of times they appear in the text.\n",
    "There is another variation of CountVectorizer with binary=True and in that case all zero entries will have 1.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_bin = CountVectorizer(binary=True)\n",
    "review_count_vec_bin = count_vec_bin.fit_transform(corpus)\n",
    "\n",
    "review_count_vec_bin.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.11470787, 0.05735393, 0.05735393,\n",
       "        0.3441236 , 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.11470787, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.11470787, 0.05735393,\n",
       "        0.05735393, 0.11470787, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.11470787, 0.05735393, 0.1720618 , 0.05735393, 0.05735393,\n",
       "        0.11470787, 0.05735393, 0.05735393, 0.11470787, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.11470787, 0.05735393, 0.05735393, 0.05735393, 0.11470787,\n",
       "        0.05735393, 0.05735393, 0.11470787, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.3441236 , 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.1720618 , 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.11470787, 0.05735393, 0.05735393, 0.11470787, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.22941573, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.11470787, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.11470787, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.05735393,\n",
       "        0.05735393, 0.05735393, 0.05735393, 0.05735393, 0.22941573,\n",
       "        0.05735393, 0.11470787, 0.05735393, 0.11470787, 0.05735393]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec=TfidfVectorizer()\n",
    "review_tfidf_vec=tfidf_vec.fit_transform(corpus)\n",
    "review_tfidf_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train,dataset_test,train_data_label,test_data_label=train_test_split(df['review'],df['sentiment'],test_size=.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Convert sentiments to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label = (train_data_label.replace({'positive': 1, 'negative': 0})).values\n",
    "test_data_label  = (test_data_label.replace({'positive': 1, 'negative': 0})).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mycom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus_train=[]\n",
    "corpus_test=[]\n",
    "for i in range(dataset_train.shape[0]):\n",
    "    soup = BeautifulSoup(dataset_train.iloc[i], \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    lem = WordNetLemmatizer()\n",
    "    review = [lem.lemmatize(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus_train.append(review)\n",
    "    \n",
    "for j in range(dataset_test.shape[0]):\n",
    "    soup = BeautifulSoup(dataset_test.iloc[j], \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    lem = WordNetLemmatizer()\n",
    "    review = [lem.lemmatize(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decent movie although little bit short time pack lot action grit commonsense emotion time frame matt dillon main character great job movie emotion intensity convincing tense throughout movie typical fancy expensive hollywood cgi action movie satisfying movie indeed price evening great movie movie straight traditional action movie great acting story directing would recommend movie character development character good make believe actually seeing real event taking place movie believe made cheaper budget acting quality much higher'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We will now vectorize using TF-IDF technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##I am going to use LinearSVC as my first model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "Linear_svc=LinearSVC(C=.5,random_state=42)\n",
    "Linear_svc.fit(tfidf_vec_train,train_data_label)\n",
    "predict = Linear_svc.predict(tfidf_vec_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.88      0.90      7411\n",
      "    Positive       0.89      0.92      0.90      7589\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6558  853]\n",
      " [ 638 6951]]\n",
      "Accuracy: \n",
      " 0.9006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Now we will vectorize using CountVectorizer(binary=True) and fit it on LinearSVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_vec = CountVectorizer(ngram_range=(1, 3), binary=True)\n",
    "ind_vec_train = ind_vec.fit_transform(corpus_train)\n",
    "ind_vec_test = ind_vec.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mycom\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "linear_svc_ind = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc_ind.fit(ind_vec_train, train_data_label)\n",
    "\n",
    "predict_ind = linear_svc_ind.predict(ind_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      7411\n",
      "    Positive       0.89      0.90      0.90      7589\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6568  843]\n",
      " [ 759 6830]]\n",
      "Accuracy: \n",
      " 0.8932\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_ind,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_ind))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##So we are getting maximum accuracy using TF-IDF vectorizer.\n",
    "We will now fit the data to Multinomial Naive Bayes classifier. Bayesian model uses prior probabilities to predict posterior probabilites which is helpful for classification with discrete features like text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.5 GiB for an array with shape (35000, 78782) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b8363bb9c504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtfidf_vec_test_NB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vec_NB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vec_train_NB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_vec_test_NB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 20.5 GiB for an array with shape (35000, 78782) and data type float64"
     ]
    }
   ],
   "source": [
    "tfidf_vec_NB = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vec_train_NB = tfidf_vec_NB.fit_transform(corpus_train)\n",
    "\n",
    "tfidf_vec_test_NB = tfidf_vec_NB.transform(corpus_test)\n",
    "\n",
    "print(tfidf_vec_train_NB.toarray().shape, tfidf_vec_test_NB.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#So there are 81301 terms in the corpus and we will use a Chi-Square test to select top 50000 features.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=50000)\n",
    "tfidf_vec_train_NB = ch2.fit_transform(tfidf_vec_train_NB, train_data_label)\n",
    "tfidf_vec_test_NB  = ch2.transform(tfidf_vec_test_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We can see the top features as wel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acquired'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vec_NB.get_feature_names()\n",
    "feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "feature_names = np.asarray(feature_names)\n",
    "feature_names[322]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's fit the data to Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multi_clf = MultinomialNB()\n",
    "multi_clf.fit(tfidf_vec_train_NB, train_data_label)\n",
    "\n",
    "predict_NB = multi_clf.predict(tfidf_vec_test_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.88      0.87      7411\n",
      "    Positive       0.88      0.85      0.86      7589\n",
      "\n",
      "    accuracy                           0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.87      0.86      0.86     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6519  892]\n",
      " [1139 6450]]\n",
      "Accuracy: \n",
      " 0.8646\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. We will now use CountVectorizer as our Bag-of-Words model before applying MultinomialNB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_NB = CountVectorizer(ngram_range=(1, 3), binary=False)\n",
    "count_vec_train_NB = count_vec_NB.fit_transform(corpus_train)\n",
    "count_vec_test_NB = count_vec_NB.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clf_count = MultinomialNB()\n",
    "multi_clf_count.fit(count_vec_train_NB, train_data_label)\n",
    "\n",
    "predict_NB_count = multi_clf_count.predict(count_vec_test_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.89      0.89      7411\n",
      "    Positive       0.89      0.88      0.89      7589\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6625  786]\n",
      " [ 909 6680]]\n",
      "Accuracy: \n",
      " 0.887\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB_count,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB_count))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###So LinearSVC using TF-IDF vectorization gives the maximum accuracy and we can see the outcome on our makeshift test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  I really liked this Summerslam due to the look...\n",
       "1  Not many television shows appeal to quite as m...\n",
       "2  The film quickly gets to a major chase scene w...\n",
       "3  Jane Austen would definitely approve of this o...\n",
       "4  Expectations were somewhat high for me when I ..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_predict = dataset_test.copy()\n",
    "dataset_predict = pd.DataFrame(dataset_predict)\n",
    "dataset_predict.columns = ['review']\n",
    "dataset_predict = dataset_predict.reset_index()\n",
    "dataset_predict = dataset_predict.drop(['index'], axis=1)\n",
    "dataset_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual_label = test_data_label.copy()\n",
    "test_actual_label = pd.DataFrame(test_actual_label)\n",
    "test_actual_label.columns = ['sentiment']\n",
    "test_actual_label['sentiment'] = test_actual_label['sentiment'].replace({1: 'positive', 0: 'negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_label = predict.copy()\n",
    "test_predicted_label = pd.DataFrame(test_predicted_label)\n",
    "test_predicted_label.columns = ['predicted_sentiment']\n",
    "test_predicted_label['predicted_sentiment'] = test_predicted_label['predicted_sentiment'].replace({1: 'positive', 0: 'negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  I really liked this Summerslam due to the look...  positive   \n",
       "1  Not many television shows appeal to quite as m...  positive   \n",
       "2  The film quickly gets to a major chase scene w...  negative   \n",
       "3  Jane Austen would definitely approve of this o...  positive   \n",
       "4  Expectations were somewhat high for me when I ...  negative   \n",
       "\n",
       "  predicted_sentiment  \n",
       "0            negative  \n",
       "1            positive  \n",
       "2            negative  \n",
       "3            positive  \n",
       "4            negative  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = pd.concat([dataset_predict, test_actual_label, test_predicted_label], axis=1)\n",
    "test_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
